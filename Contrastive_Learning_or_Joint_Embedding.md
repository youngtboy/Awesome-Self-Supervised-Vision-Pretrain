## Contrastive Learing/Joint Embedding
This is a paper list of Contrastive Learing/Joint Embedding based self-supervised pretraining method. All papers are listed in order of their appearance in arxiv. * indicates that this method combines Masked Image Modeling and Contrastive Learing/Joint Embedding methods.

## 2020

- **[MoCov1]** ðŸŒŸ Momentum Contrast for Unsupervised Visual Representation Learningn | **[CVPR'20]** |[`[paper]`](https://arxiv.org/abs/1911.05722) [`[code]`](https://github.com/facebookresearch/moco) 
   <details close>
   <summary>MoCov1 Arch</summary>
   <p align="center"><img width="50%" src="https://user-images.githubusercontent.com/66102178/278521451-0db281de-9c7c-4292-8e52-2fe125fb4afa.png" /></p>
   </details>

- **[SimCLRv1]** ðŸŒŸ A Simple Framework for Contrastive Learning of Visual Representations | **[ICML'20]** | [`[paper]`](https://arxiv.org/abs/2002.05709) [`[code]`](https://github.com/google-research/simclr)
   <details close>
   <summary>SimCLRv1 Arch</summary>
   <p align="center"><img width="50%" src="https://user-images.githubusercontent.com/66102178/278521501-9b04ab1f-e359-4a52-8b0a-c3a794bac1cb.png" /></p>
   </details>

- **[MoCov2]** Improved Baselines with Momentum Contrastive Learning | **[arxiv'20]** | [`[paper]`](https://arxiv.org/abs/2003.04297) [`[code]`](https://github.com/facebookresearch/moco)
   <details close>
   <summary>MoCov2 Arch</summary>
   <p align="center"><img width="50%" src="https://user-images.githubusercontent.com/66102178/278521477-0f1bacd5-6a46-4c5f-bfac-de6f8ec8898d.png" /></p>
   </details>

- **[BYOL]** Bootstrap your own latent: A new approach to self-supervised Learning | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.07733) [`[code]`](https://github.com/deepmind/deepmind-research)
   <details close>
   <summary>BYOL Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278522794-990ed1bf-4e4c-4f2b-8346-225ac84b325a.png" /></p>
   </details>

- **[SimCLRv2]** Big Self-Supervised Models are Strong Semi-Supervised Learners | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.10029) [`[code]`](https://github.com/google-research/simclr)
   <details close>
   <summary>SimCLRv2 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278523316-ae6fc7b3-3908-4373-8fe2-21ec808fa9a0.png" /></p>
   </details>

- **[SwAV]** Unsupervised Learning of Visual Features by Contrasting Cluster Assignments | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.09882) [`[code]`](https://github.com/facebookresearch/swav)
   <details close>
   <summary>SwAV Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278523332-c1f8a4fb-2c1a-4d93-8465-de449e9be23e.png" /></p>
   </details>

- **[RELICv1]** Representation Learning via Invariant Causal Mechanisms | **[ICLR'21]** | [`[paper]`](https://arxiv.org/abs/2010.07922)

- **[DenseCL]** Dense Contrastive Learning for Self-Supervised Visual Pre-Training | **[CVPR'21]** | [`[paper]`](https://arxiv.org/abs/2011.09157) [`[code]`](https://github.com/WXinlong/DenseCL)
   <details close>
   <summary>DenseCL Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278523967-c201aa64-d163-43a2-b5e7-a0f43d81b083.png" /></p>
   </details>

- **[SimSiam]** ðŸŒŸ Exploring Simple Siamese Representation Learning | **[CVPR'21]** [`[paper]`](https://arxiv.org/abs/2011.10566) [`[code]`](https://github.com/facebookresearch/simsiam)
   <details close>
   <summary>SimSiam Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/278523977-fa990e64-08af-44d6-a7ce-cbdf5766bd0b.png" /></p>
   </details>

## 2021

- **[CLIP]** ðŸŒŸ Learning Transferable Visual Models From Natural Language Supervision | **[ICML'21]** | [`[paper]`](https://arxiv.org/abs/2103.00020) [`[code]`](https://github.com/OpenAI/CLIP)
   <details close>
   <summary>CLIP Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527705-9e2961d5-76d3-4b55-8af1-dc52854124b4.png" /></p>
   </details>

- **[Barlow Twins]** Barlow Twins: Self-Supervised Learning via Redundancy Reduction | **[ICML'21]** | [`[paper]`](https://arxiv.org/abs/2103.03230) [`[code]`](https://github.com/facebookresearch/barlowtwins)
   <details close>
   <summary>Barlow Twins Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527722-e8974a70-a503-4413-8a74-550c4af73e76.png" /></p>
   </details>

- **[MoCov3]** ðŸŒŸ An Empirical Study of Training Self-Supervised Vision Transformers | **[ICCV'21]** | [`[paper]`](https://arxiv.org/abs/2104.02057) [`[code]`](https://github.com/facebookresearch/moco-v3)

- **[DINOv1]** ðŸŒŸ Emerging Properties in Self-Supervised Vision Transformers | **[ICCV'21]** |[`[paper]`](https://arxiv.org/abs/2104.14294) [`[code]`](https://github.com/facebookresearch/dino)
   <details close>
   <summary>DINOv1 Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/278527957-77176a63-50ae-4048-b939-33b1ff76235c.png" /></p>
   </details>

- **[VICReg]** VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2105.04906) [`[code]`](https://github.com/facebookresearch/vicreg)
   <details close>
   <summary>VICReg Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527974-00fa13fc-fca9-4922-a56a-b25e325bf4bf.png" /></p>
   </details>

- **[MST]*** MST: Masked Self-Supervised Transformer for Visual Representation | **[NIPS'21]** | [`[paper]`](https://arxiv.org/abs/2106.05656) 
   <details close>
   <summary>MST Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278527994-ef413def-2a1d-49bf-ba71-37b64f860155.png" /></p>
   </details>

- **[C-BYOL/C-SimLCR]** Compressive Visual Representations | **[NIPS'21]** | [`[paper]`](https://arxiv.org/abs/2109.12909) [`[code]`](https://github.com/google-research/compressive-visual-representations)

- **[iBOT]*** iBOT: Image BERT Pre-Training with Online Tokenizer | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2111.07832) [`[code]`](https://github.com/bytedance/ibot)
   <details close>
   <summary>iBOT Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278535707-7d10a773-5ca6-4cb7-9e57-99191fa0617f.png" /></p>
   </details>

## 2022

- **[RELICv2]** Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet? | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2201.05119)
   <details close>
   <summary>RELICv2 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278783597-b0ae0798-96d3-4e88-9d8c-73cbdefa5f30.png" /></p>
   </details>

- **[RePre]** RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2201.06857)
   <details close>
   <summary>RePre Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783635-70fe848a-8613-44d2-ad25-7fddcd5ebc86.png" /></p>
   </details>

- **[FD]*** Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14141) [`[code]`](https://github.com/SwinTransformer/Feature-Distillation)
   <details close>
   <summary>FD Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/279017061-3e545bb4-aef7-4eb8-93a2-6e22e6bb1668.png" /></p>
   </details>

- **[SIM]*** Siamese Image Modeling for Self-Supervised Vision Representation Learning | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2206.01204) [`[code]`](https://github.com/OpenGVLab/Siamese-Image-Modeling)
   <details close>
   <summary>SIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017191-9689d5b7-aab8-4e9f-99b9-386bcdcf46e5.png" /></p>
   </details>

- **[CMAE]*** Contrastive Masked Autoencoders are Stronger Vision Learners | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2207.13532) [`[code]`](https://github.com/ZhichengHuang/CMAE)
   <details close>
   <summary>CMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017308-51b1ffeb-eaec-4b30-bb7a-2bed94a5f6f3.png" /></p>
   </details>

- **[MaskCLIP]*** MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2208.12262) [`[code]`](https://github.com/LightDXY/MaskCLIP)
   <details close>
   <summary>BEiTv3 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017487-ac4c4cef-fd3e-423f-bc91-0d4e0c6e1925.png" /></p>
   </details>

- **[MimCo]*** MimCo: Masked Image Modeling Pre-training with Contrastive Teacher | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2209.03063)
   <details close>
   <summary>MimCo Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017531-451d29ec-0e8f-490f-9f8d-68908fe8671d.png" /></p>
   </details>

- **[VICRegL]** VICRegL: Self-Supervised Learning of Local Visual Features | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2210.01571) [`[code]`](https://github.com/facebookresearch/VICRegL)
   <details close>
   <summary>VICRegL Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017555-c853ba70-b6b5-4f42-8df7-b6de044921b9.png" /></p>
   </details>

- **[CAN]*** A simple, efficient and scalable contrastive masked autoencoder for learning visual representations | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2210.16870) [`[code]`](https://github.com/shlokk/mae-contrastive)
   <details close>
   <summary>CAN Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293610-efa98556-8cfd-4184-908b-af47ad88f01d.png" /></p>
   </details>

- **[SCFS]** Semantics-Consistent Feature Search for Self-Supervised Visual Representation Learning | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2212.06486) [`[code]`](https://github.com/skyoux/scfs)
   <details close>
   <summary>SCFS Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293684-8f45ed8d-153a-48e6-b328-3cf80131bbdc.png" /></p>
   </details>

## 2023

- **[I-JEPA]** Self-SupervisedÂ LearningÂ fromÂ ImagesÂ with aÂ Joint-EmbeddingÂ PredictiveÂ Architecture | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2301.08243) [`[code]`](https://github.com/facebookresearch/ijepa)
   <details close>
   <summary>I-JEPA Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/279293807-e1f5fdbc-ba14-4902-b70a-a9c577dbb7e9.png" /></p>
   </details>

- **[Layer Grafted]*** Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2302.14138) [`[code]`](https://github.com/VITA-Group/layerGraftedPretraining_ICLR23)
   <details close>
   <summary>Layer Grafted Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293841-f7f38afa-6121-408c-8d43-a5e1c3f5c10c.png" /></p>
   </details>

- **[EMP]** EMP-SSL: Towards Self-Supervised Learning in One Training Epoch | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2304.03977) [`[code]`](https://github.com/tsb0601/EMP-SSL)
   <details close>
   <summary>EMP Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293962-94c6168d-4e60-4883-b9b7-f9e2e662863a.png" /></p>
   </details>

- **[DINOv2]** DINOv2ï¼šLearning Robust Visual Features without Supervision | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2304.07193) [`[code]`](https://github.com/facebookresearch/dinov2)

- **[CL-vs-MIM]*** What Do Self-Supervised Vision Transformers Learn? | **[ICLR'23]** |[`[paper]`](https://arxiv.org/abs/2305.00729) [`[code]`](https://github.com/naver-ai/cl-vs-mim)

- **[ccMIM]*** Contextual Image Masking Modeling via Synergized Contrasting without View Augmentation for Faster and Better Visual Pretraining | **[ICLR'23]** | [`[paper]`](https://openreview.net/pdf?id=A3sgyt4HWp) [`[code]`](https://github.com/Sherrylone/ccMIM)
   <details close>
   <summary>ccMIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279294010-0d90109e-bab9-4377-9848-19efcdf73f8a.png" /></p>
   </details>

- **[DreamTeacher]** DreamTeacher: Pretraining Image Backbones with Deep Generative Models | **[ICCV'23]** |[`[paper]`](https://arxiv.org/abs/2307.07487)
   <details close>
   <summary>DreamTeacher Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279294020-e461e141-1ccb-471f-9520-bf880c615843.png" /></p>
   </details>
