## Masked Image Modeling
This is a paper list of Masked Image Modeling based self-supervised pretraining method. All papers are listed in order of their appearance in arxiv. * indicates that this method combines Masked Image Modeling and Contrastive Learing/Joint Embedding methods.

## 2021

- **[MST]*** MST: Masked Self-Supervised Transformer for Visual Representation | **[NIPS'21]** | [`[paper]`](https://arxiv.org/abs/2106.05656) 
   <details close>
   <summary>MST Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278527994-ef413def-2a1d-49bf-ba71-37b64f860155.png" /></p>
   </details>

- **[BEiTv1]** ðŸŒŸ BEiT: BERT Pre-Training of Image Transformers | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2106.08254) [`[code]`](https://github.com/microsoft/unilm)
   <details close>
   <summary>BEiTv1 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278530019-f87113af-3180-4e51-9801-57d8ec426c0c.png" /></p>
   </details>

- **[MAE]** ðŸŒŸ Masked Autoencoders Are Scalable Vision Learners | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2111.06377) [`[code]`](https://github.com/facebookresearch/mae)
   <details close>
   <summary>MAE Arch</summary>
   <p align="center"><img width="80%" src="https://user-images.githubusercontent.com/66102178/278535696-62557901-d082-4f49-9a24-10d81e06c649.png" /></p>
   </details>

- **[iBOT]*** iBOT: Image BERT Pre-Training with Online Tokenizer | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2111.07832) [`[code]`](https://github.com/bytedance/ibot)
   <details close>
   <summary>iBOT Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278535707-7d10a773-5ca6-4cb7-9e57-99191fa0617f.png" /></p>
   </details>

- **[SimMIM]** ðŸŒŸ SimMIM: A Simple Framework for Masked Image Modeling | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2111.09886) [`[code]`](https://github.com/microsoft/SimMIM)
   <details close>
   <summary>SimMIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278535722-70041e7f-3da3-4852-8780-f932903c615e.png" /></p>
   </details>

- **[PeCo]** PeCoï¼šPerceptual Codebook for BERT Pre-training of Vision Transformers | **[AAAI'23]** | [`[paper]`](https://arxiv.org/abs/2111.12710)
   <details close>
   <summary>PeCo Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278535737-2f090bd3-5589-4ffb-8c97-31353ac707e9.png" /></p>
   </details>

- **[MaskFeat]** Masked Feature Prediction for Self-Supervised Visual Pre-Training | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2112.09133) [`[code]`](https://github.com/facebookresearch/SlowFast)
   <details close>
   <summary>MaskFeat Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278535752-bd201a87-1687-4e2e-b2d6-d45fdb14ec13.png" /></p>
   </details>

## 2022

- **[CAEv1]** Context Autoencoder for Self-Supervised Representation Learning | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2202.03026) [`[code]`](https://github.com/lxtGH/CAE)
   <details close>
   <summary>CAEv1 Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783643-8b4d11fa-acd7-4d71-b02c-075f1917f8c1.png" /></p>
   </details>

- **[CIM]** Corrupted Image Modeling for Self-Supervised Visual Pre-Training | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2202.03382)
   <details close>
   <summary>CIM Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783648-c93a3c22-b10f-4a53-a89d-25e4a52695da.png" /></p>
   </details>

- **[MVP]** MVP: Multimodality-guided Visual Pre-training | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2203.05175)
   <details close>
   <summary>MVP Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783650-1915f976-cd80-4dba-a8b4-4bc357dace1e.png" /></p>
   </details>

- **[ConvMAE]** ConvMAE: Masked Convolution Meets Masked Autoencoders | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2205.03892) [`[code]`](https://github.com/Alpha-VL/ConvMAE)
   <details close>
   <summary>ConvMAE Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783657-82d3f0a5-4f5d-4767-a720-9b51d6b2704d.png" /></p>
   </details>

- **[ConMIM]*** Masked Image Modeling with Denoising Contrast | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2205.09616) [`[code]`](https://github.com/TencentARC/ConMIM)
   <details close>
   <summary>ConMIM Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783673-1a3be697-056e-4346-a204-a56904ef525e.png" /></p>
   </details>

- **[MixMAE]** MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers | **[CVPR'23]** |[`[paper]`](https://arxiv.org/abs/2205.13137) [`[code]`](https://github.com/Sense-X/MixMIM)
   <details close>
   <summary>MixMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278783687-7c70612e-8d22-44f1-a58e-2fbc3b74094e.png" /></p>
   </details>

- **[A2MIM]** Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN | **[ICML'23]** | [`[paper]`](https://arxiv.org/abs/2205.13943) [`[code]`](https://github.com/Westlake-AI/A2MIM)
   <details close>
   <summary>A2MIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017037-cfbc32c1-ddf6-4436-a2aa-3521106616a1.png" /></p>
   </details>

- **[FD]*** Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14141) [`[code]`](https://github.com/SwinTransformer/Feature-Distillation)
   <details close>
   <summary>FD Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/279017061-3e545bb4-aef7-4eb8-93a2-6e22e6bb1668.png" /></p>
   </details>

- **[ObjMAE]** Object-wise Masked Autoencoders for Fast Pre-training | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14338)
   <details close>
   <summary>ObjMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017077-f987df1f-7607-4fa6-bebc-7253829c058e.png" /></p>
   </details>

- **[SupMAE]** SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14540) [`[code]`](https://github.com/enyac-group/supmae)
   <details close>
   <summary>SupMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017126-3784511f-7583-4c09-9610-90775877a9d9.png" /></p>
   </details>

- **[HiViT]** HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2205.14949) [`[mmpretrian code]`](https://github.com/open-mmlab/mmpretrain)
   <details close>
   <summary>HiViT Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017144-18dbf120-f8a7-4149-b651-3fedc3269122.png" /></p>
   </details>

- **[LoMaR]** Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2206.00790) [`[code]`](https://github.com/junchen14/LoMaR)
   <details close>
   <summary>LoMaR Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017165-7854db74-446b-42c6-96d7-f2d89b8e8ccc.png" /></p>
   </details>

- **[SIM]*** Siamese Image Modeling for Self-Supervised Vision Representation Learning | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2206.01204) [`[code]`](https://github.com/OpenGVLab/Siamese-Image-Modeling)
   <details close>
   <summary>SIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017191-9689d5b7-aab8-4e9f-99b9-386bcdcf46e5.png" /></p>
   </details>

- **[MFM]** Masked Frequency Modeling for Self-Supervised Visual Pre-Training | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2206.07706) [`[code]`](https://github.com/Jiahao000/MFM)
   <details close>
   <summary>MFM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017228-86469272-c029-468b-ab0e-fb589bafd3bc.png" /></p>
   </details>

- **[BootMAE]** Bootstrapped Masked Autoencoders for Vision BERT Pretraining | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2207.07116) [`[code]`](https://github.com/LightDXY/BootMAE)
   <details close>
   <summary>BootMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017260-45a0d61c-7d40-4a83-9849-34ed8d1cddd8.png" /></p>
   </details>

- **[CMAE]*** Contrastive Masked Autoencoders are Stronger Vision Learners | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2207.13532) [`[code]`](https://github.com/ZhichengHuang/CMAE)
   <details close>
   <summary>CMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017308-51b1ffeb-eaec-4b30-bb7a-2bed94a5f6f3.png" /></p>
   </details>

- **[SdAE]** SdAE: Self-distillated Masked Autoencoder | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2208.00449) [`[code]`](https://github.com/AbrahamYabo/SdAE)
   <details close>
   <summary>SdAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017380-961241a1-af11-4962-8528-383a9c6f2b04.png" /></p>
   </details>

- **[MILAN]** MILAN: Masked Image Pretraining on Language Assisted Representation | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2208.06049) [`[code]`](https://github.com/zejiangh/MILAN)
   <details close>
   <summary>MILAN Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017407-2975d5f9-aa9f-4e74-b060-706c351d9147.png" /></p>
   </details>

- **[BEiTv2]** BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2208.06366) [`[code]`](https://github.com/microsoft/unilm)
   <details close>
   <summary>BEiTv2 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017794-e9976a9d-f85c-4167-b425-12b8c3a5d845.png" /></p>
    <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017814-8b4aab8f-19ba-44b9-a714-6ddbb8de7668.png" /></p>
   </details>

- **[BEiTv3]** Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2208.10442) [`[code]`](https://github.com/microsoft/unilm)
   <details close>
   <summary>BEiTv3 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017449-107f3b81-264b-4af5-aae1-e3444e33a436.png" /></p>
   </details>

- **[MaskCLIP]*** MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2208.12262) [`[code]`](https://github.com/LightDXY/MaskCLIP)
   <details close>
   <summary>BEiTv3 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017487-ac4c4cef-fd3e-423f-bc91-0d4e0c6e1925.png" /></p>
   </details>

- **[MimCo]*** MimCo: Masked Image Modeling Pre-training with Contrastive Teacher | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2209.03063)
   <details close>
   <summary>MimCo Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017531-451d29ec-0e8f-490f-9f8d-68908fe8671d.png" /></p>
   </details>

- **[U-MAE]** How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2210.08344) [`[code]`](https://github.com/zhangq327/U-MAE)

- **[i-MAE]** i-MAE: Are Latent Representations in Masked Autoencoders Linearly Separable? | **[axiv'22]** | [`[paper]`](https://arxiv.org/abs/2210.11470) [`[code]`](https://github.com/VILA-Lab/i-mae)
   <details close>
   <summary>i-MAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293573-650c324f-5580-42e6-857f-33a9fde517e6.png" /></p>
   </details>

- **[CAN]*** A simple, efficient and scalable contrastive masked autoencoder for learning visual representations | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2210.16870) [`[code]`](https://github.com/shlokk/mae-contrastive)
   <details close>
   <summary>CAN Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293610-efa98556-8cfd-4184-908b-af47ad88f01d.png" /></p>
   </details>

- **[EVA]** EVA: Exploring the Limits of Masked Visual Representation Learning at Scale | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2211.07636) [`[code]`](https://github.com/baaivision/EVA)
   <details close>
   <summary>EVA Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/279293632-2b2397ac-ab5c-4c7d-8896-7200bb15d95b.png" /></p>
   </details>


- **[CAEv2]** CAE v2: Context Autoencoder with CLIP Target | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2211.09799)
   <details close>
   <summary>CAEv2 Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/279293651-6ca73d8c-d262-404d-93f4-5678918ddc3d.png" /></p>
   </details>

- **[iTPN]** Integrally Pre-Trained Transformer Pyramid Networks | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2211.12735) [`[code]`](https://github.com/sunsmarterjie/iTPN)
   <details close>
   <summary>iTPN Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293671-2ba443b3-5d5e-4b78-92af-ba544c7f6b6a.png" /></p>
   </details>

- **[FastMIM]** FastMIM: Expediting Masked Image Modeling Pre-training for Vision | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2212.06593) [`[code]`](https://github.com/ggjy/FastMIM.pytorch)
   <details close>
   <summary>FastMIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293703-3b6ccccf-b150-49b3-b8b8-c8977491cff7.png" /></p>
   </details>

- **[Scale-MAE]** Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2212.14532)
   <details close>
   <summary>Scale-MAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293746-3ef633f8-2878-4314-b14e-15fb6eac499e.png" /></p>
   </details>

## 2023

- **[ConvNeXtv2]** ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2301.00808) [`[code]`](https://github.com/facebookresearch/ConvNeXt-V2)
   <details close>
   <summary>ConvNeXtv2 Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/279293774-0b04c28e-8852-4704-85fd-428f58e570ed.png" /></p>
   </details>

- **[Spark]** Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2301.03580) [`[code]`](https://github.com/keyu-tian/SparK)
   <details close>
   <summary>Spark Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293789-7f106316-0635-42f6-b67f-309652b32903.png" /></p>
   </details>

- **[Layer Grafted]*** Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2302.14138) [`[code]`](https://github.com/VITA-Group/layerGraftedPretraining_ICLR23)
   <details close>
   <summary>Layer Grafted Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293841-f7f38afa-6121-408c-8d43-a5e1c3f5c10c.png" /></p>
   </details>

- **[PixMIM]** PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2303.02416) [`[code]`](https://github.com/open-mmlab/mmselfsup)
   <details close>
   <summary>PixMIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293855-868da4c3-ad67-4c36-8df0-e7ebb88fb075.png" /></p>
   </details>

- **[LocalMIM]** Masked Image Modeling with Local Multi-Scale Reconstruction | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2303.05251) [`[code]`](https://github.com/huawei-noah/Efficient-Computing/tree/master/Self-supervised/LocalMIM)
   <details close>
   <summary>LocalMIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293880-03bbb206-9c7f-42a6-80b0-c19504dc07ad.png" /></p>
   </details>
 
- **[MR-MAE]** Mimic before Reconstruct: Enhancing Masked Autoencoders with Feature Mimicking | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2303.05475)
   <details close>
   <summary>MR-MAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293915-81d80e37-7e57-4bac-889b-e0d1020fc577.png" /></p>
   </details>

- **[MixedAE]** Mixed Autoencoder for Self-supervised Visual Representation Learning | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2303.17152)
   <details close>
   <summary>MixedAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293942-ae68ace0-725a-4b4e-a494-6321dbe8dd30.png" /></p>
   </details>

- **[CL-vs-MIM]*** What Do Self-Supervised Vision Transformers Learn? | **[ICLR'23]** |[`[paper]`](https://arxiv.org/abs/2305.00729) [`[code]`](https://github.com/naver-ai/cl-vs-mim)

- **[SiamMAE]** Siamese Masked Autoencoders | **[NIPS'23]** | [`[paper]`](https://arxiv.org/abs/2305.14344)
   <details close>
   <summary>SiamMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293985-7c58e776-de93-440d-878c-f3f596dfea5a.png" /></p>
   </details>

- **[ccMIM]*** Contextual Image Masking Modeling via Synergized Contrasting without View Augmentation for Faster and Better Visual Pretraining | **[ICLR'23]** | [`[paper]`](https://openreview.net/pdf?id=A3sgyt4HWp) [`[code]`](https://github.com/Sherrylone/ccMIM)
   <details close>
   <summary>ccMIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279294010-0d90109e-bab9-4377-9848-19efcdf73f8a.png" /></p>
   </details>

- **[MFF]** Improving Pixel-based MIM by Reducing Wasted Modeling Capability | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2308.00261) [`[code]`](https://github.com/open-mmlab/mmpretrain)
   <details close>
   <summary>MFF Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279294038-118cb80c-2479-4b7e-a6b0-6e44e7d745a6.png" /></p>
   </details>

- **[DropPos]** DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions | **[NIPS'23]** | [`[paper]`](https://arxiv.org/abs/2309.03576) [`[code]`](https://github.com/Haochen-Wang409/DropPos)
   <details close>
   <summary>DropPos Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279294057-a4f7f653-18fa-4d44-ad27-f30eb4d5e02c.png" /></p>
   </details>
