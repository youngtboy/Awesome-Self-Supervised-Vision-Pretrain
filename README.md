# Self-Supervised Pretraining
This is a paper list of self-supervised pretraining method. All papers are listed in order of their appearance in arxiv. 

In addition, papers are also categorized according to different topics. You can click on the links below to get related papers on the topics you are interested in.


- [Self-Supervised Pretraining for Light-weight Model](Self-Supervised_Pretraining_for_Light-weight_Model.md)
- updating Â·Â·Â·


# All Papers

## 2020

- **[MoCov1]** ðŸŒŸ Momentum Contrast for Unsupervised Visual Representation Learningn | **[CVPR'20]** |[`[paper]`](https://arxiv.org/abs/1911.05722) [`[code]`](https://github.com/facebookresearch/moco) 
   <details close>
   <summary>MoCov1 Arch</summary>
   <p align="center"><img width="50%" src="https://user-images.githubusercontent.com/66102178/278521451-0db281de-9c7c-4292-8e52-2fe125fb4afa.png" /></p>
   </details>

- **[SimCLRv1]** ðŸŒŸ A Simple Framework for Contrastive Learning of Visual Representations | **[ICML'20]** | [`[paper]`](https://arxiv.org/abs/2002.05709) [`[code]`](https://github.com/google-research/simclr)
   <details close>
   <summary>SimCLRv1 Arch</summary>
   <p align="center"><img width="50%" src="https://user-images.githubusercontent.com/66102178/278521501-9b04ab1f-e359-4a52-8b0a-c3a794bac1cb.png" /></p>
   </details>

- **[MoCov2]** Improved Baselines with Momentum Contrastive Learning | **[arxiv'20]** | [`[paper]`](https://arxiv.org/abs/2003.04297) [`[code]`](https://github.com/facebookresearch/moco)
   <details close>
   <summary>MoCov2 Arch</summary>
   <p align="center"><img width="50%" src="https://user-images.githubusercontent.com/66102178/278521477-0f1bacd5-6a46-4c5f-bfac-de6f8ec8898d.png" /></p>
   </details>

- **[BYOL]** Bootstrap your own latent: A new approach to self-supervised Learning | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.07733) [`[code]`](https://github.com/deepmind/deepmind-research)
   <details close>
   <summary>BYOL Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278522794-990ed1bf-4e4c-4f2b-8346-225ac84b325a.png" /></p>
   </details>

- **[SimCLRv2]** Big Self-Supervised Models are Strong Semi-Supervised Learners | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.10029) [`[code]`](https://github.com/google-research/simclr)
   <details close>
   <summary>SimCLRv2 Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278523316-ae6fc7b3-3908-4373-8fe2-21ec808fa9a0.png" /></p>
   </details>

- **[SwAV]** Unsupervised Learning of Visual Features by Contrasting Cluster Assignments | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.09882) [`[code]`](https://github.com/facebookresearch/swav)
   <details close>
   <summary>SwAV Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278523332-c1f8a4fb-2c1a-4d93-8465-de449e9be23e.png" /></p>
   </details>

- **[RELICv1]** Representation Learning via Invariant Causal Mechanisms | **[ICLR'21]** | [`[paper]`](https://arxiv.org/abs/2010.07922)

- **[CompRess]** CompRess: Self-Supervised Learning by Compressing Representations | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2010.14713) [`[code]`](https://github.com/UMBCvision/CompReSS)
   <details close>
   <summary>CompRess Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278523953-090209e4-5af6-4fd8-9b3a-45552fec3fef.png" /></p>
   </details>

- **[DenseCL]** Dense Contrastive Learning for Self-Supervised Visual Pre-Training | **[CVPR'21]** | [`[paper]`](https://arxiv.org/abs/2011.09157) [`[code]`](https://github.com/WXinlong/DenseCL)
   <details close>
   <summary>DenseCL Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278523967-c201aa64-d163-43a2-b5e7-a0f43d81b083.png" /></p>
   </details>

- **[SimSiam]** ðŸŒŸ Exploring Simple Siamese Representation Learning | **[CVPR'21]** [`[paper]`](https://arxiv.org/abs/2011.10566) [`[code]`](https://github.com/facebookresearch/simsiam)
   <details close>
   <summary>SimSiam Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278523977-fa990e64-08af-44d6-a7ce-cbdf5766bd0b.png" /></p>
   </details>

## 2021

- **[SEED]** SEED: Self-supervised Distillation For Visual Representation | **[ICLR'21]** | [`[paper]`](https://arxiv.org/abs/2101.04731) [`[code]`](https://github.com/zhangyifei01/SEED_ICLR21)
   <details close>
   <summary>SEED Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278526068-55cb1d88-2d59-44a3-a998-a2238886f4db.png" /></p>
   </details>

- **[CLIP]** ðŸŒŸ Learning Transferable Visual Models From Natural Language Supervision | **[ICML'21]** | [`[paper]`](https://arxiv.org/abs/2103.00020) [`[code]`](https://github.com/OpenAI/CLIP)
   <details close>
   <summary>CLIP Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527705-9e2961d5-76d3-4b55-8af1-dc52854124b4.png" /></p>
   </details>

- **[Barlow Twins]** Barlow Twins: Self-Supervised Learning via Redundancy Reduction | **[ICML'21]** | [`[paper]`](https://arxiv.org/abs/2103.03230) [`[code]`](https://github.com/facebookresearch/barlowtwins)
   <details close>
   <summary>Barlow Twins Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527722-e8974a70-a503-4413-8a74-550c4af73e76.png" /></p>
   </details>

- **[S3L]** Rethinking Self-Supervised Learning: Small is Beautiful | **[arxiv'21]** | [`[paper]`](https://arxiv.org/abs/2103.13559) [`[code]`](https://github.com/CupidJay/Scaled-down-self-supervised-learning)

- **[MoCov3]** ðŸŒŸ An Empirical Study of Training Self-Supervised Vision Transformers | **[ICCV'21]** | [`[paper]`](https://arxiv.org/abs/2104.02057) [`[code]`](https://github.com/facebookresearch/moco-v3)

- **[DisCo]** DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2104.09124) [`[code]`](https://github.com/Yuting-Gao/DisCo-pytorch)
   <details close>
   <summary>DisCo Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527927-36c1a899-140d-406e-bb26-251dd32fe3e4.png" /></p>
   </details>

- **[DoGo]** Distill on the Go: Online knowledge distillation in self-supervised learning | **[CVPRW'21]** | [`[paper]`](https://arxiv.org/abs/2104.09866) [`[code]`](https://github.com/NeurAI-Lab/DoGo)
   <details close>
   <summary>DisCo Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527946-df193c6f-f4e0-4372-b0c1-b8e9af397cc2.png" /></p>
   </details>

- **[DINOv1]** ðŸŒŸ Emerging Properties in Self-Supervised Vision Transformers | **[ICCV'21]** |[`[paper]`](https://arxiv.org/abs/2104.14294) [`[code]`](https://github.com/facebookresearch/dino)
   <details close>
   <summary>DINOv1 Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527957-77176a63-50ae-4048-b939-33b1ff76235c.png" /></p>
   </details>

- **[VICReg]** VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2105.04906) [`[code]`](https://github.com/facebookresearch/vicreg)
   <details close>
   <summary>VICReg Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527974-00fa13fc-fca9-4922-a56a-b25e325bf4bf.png" /></p>
   </details>

- **[MST]** MST: Masked Self-Supervised Transformer for Visual Representation | **[NIPS'21]** | [`[paper]`](https://arxiv.org/abs/2106.05656) 
   <details close>
   <summary>MST Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527994-ef413def-2a1d-49bf-ba71-37b64f860155.png" /></p>
   </details>

- **[BEiTv1]** ðŸŒŸ BEiT: BERT Pre-Training of Image Transformers | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2106.08254) [`[code]`](https://github.com/microsoft/unilm)
   <details close>
   <summary>BEiTv1 Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278530019-f87113af-3180-4e51-9801-57d8ec426c0c.png" /></p>
   </details>

- **[SimDis]** Simple Distillation Baselines for Improving Small Self-supervised Models | **[ICCVW'21]** | [`[paper]`](https://arxiv.org/abs/2106.11304) [`[code]`](https://github.com/JindongGu/SimDis)
   <details close>
   <summary>SimDis Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278535661-b303cfbe-7a43-4fe3-88cc-babee0a52346.png" /></p>
   </details>

- **[OSS]** Unsupervised Representation Transfer for Small Networks: I Believe I Can Distill On-the-Fly | **[NIPS'21]** | [`[paper]`](https://proceedings.neurips.cc/paper_files/paper/2021/hash/cecd845e3577efdaaf24eea03af4c033-Abstract.html)
   <details close>
   <summary>OSS Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278535677-e0169f0f-4461-4e9f-bf96-fa72246a2f4f.png" /></p>
   </details>

- **[BINGO]** Bag of Instances Aggregation Boosts Self-supervised Distillation | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2107.01691) [`[code]`](https://github.com/haohang96/bingo)
   <details close>
   <summary>BINGO Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278535686-1f9b8ec7-66ac-4735-aad2-6adaa7615f51.png" /></p>
   </details>

- **[SSL-Small]** On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals | **[AAAI'22]** | [`[paper]`](https://arxiv.org/abs/2107.14762) [`[code]`](https://github.com/WOWNICE/ssl-small)

- **[C-BYOL/C-SimLCR]** Compressive Visual Representations | **[NIPS'21]** | [`[paper]`](https://arxiv.org/abs/2109.12909) [`[code]`](https://github.com/google-research/compressive-visual-representations)

- **[MAE]** ðŸŒŸ Masked Autoencoders Are Scalable Vision Learners | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2111.06377) [`[code]`](https://github.com/facebookresearch/mae)
   <details close>
   <summary>MAE Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278535696-62557901-d082-4f49-9a24-10d81e06c649.png" /></p>
   </details>

- **[iBOT]** iBOT: Image BERT Pre-Training with Online Tokenizer | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2111.07832) [`[code]`](https://github.com/bytedance/ibot)
   <details close>
   <summary>iBOT Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278535707-7d10a773-5ca6-4cb7-9e57-99191fa0617f.png" /></p>
   </details>

- **[SimMIM]** ðŸŒŸ SimMIM: A Simple Framework for Masked Image Modeling | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2111.09886) [`[code]`](https://github.com/microsoft/SimMIM)
   <details close>
   <summary>SimMIM Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278535722-70041e7f-3da3-4852-8780-f932903c615e.png" /></p>
   </details>

- **[PeCo]** PeCoï¼šPerceptual Codebook for BERT Pre-training of Vision Transformers | **[AAAI'23]** | [`[paper]`](https://arxiv.org/abs/2111.12710)
   <details close>
   <summary>PeCo Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278535737-2f090bd3-5589-4ffb-8c97-31353ac707e9.png" /></p>
   </details>

- **[MaskFeat]** Masked Feature Prediction for Self-Supervised Visual Pre-Training | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2112.09133) [`[code]`](https://github.com/facebookresearch/SlowFast)
   <details close>
   <summary>MaskFeat Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278535752-bd201a87-1687-4e2e-b2d6-d45fdb14ec13.png" /></p>
   </details>

## 2022

- **[RELICv2]** Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet? | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2201.05119)

- **[SimReg]** SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation | **[BMVC'21]** | [`[paper]`](https://arxiv.org/abs/2201.05131) [`[code]`](https://github.com/UCDvision/simreg)

- **[RePre]** RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2201.06857)

- **[CAE]** Context Autoencoder for Self-Supervised Representation Learning | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2202.03026) [`[code]`](https://github.com/lxtGH/CAE)

- **[CIM]** Corrupted Image Modeling for Self-Supervised Visual Pre-Training | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2202.03382)

- **[MVP]** MVP: Multimodality-guided Visual Pre-training | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2203.05175)

- **[ConvMAE]** ConvMAE: Masked Convolution Meets Masked Autoencoders | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2205.03892) [`[code]`](https://github.com/Alpha-VL/ConvMAE)

- **[ConMIM]** Masked Image Modeling with Denoising Contrast | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2205.09616) [`[code]`](https://github.com/TencentARC/ConMIM)

- **[MixMAE]** MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers | **[CVPR'23]** |[`[paper]`](https://arxiv.org/abs/2205.13137) [`[code]`](https://github.com/Sense-X/MixMIM)

- **[A2MIM]** Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN | **[ICML'23]** | [`[paper]`](https://arxiv.org/abs/2205.13943) [`[code]`](https://github.com/Westlake-AI/A2MIM)

- **[FD]** Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14141) [`[code]`](https://github.com/SwinTransformer/Feature-Distillation)

- **[ObjMAE]** Object-wise Masked Autoencoders for Fast Pre-training | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14338)

- **[MAE-Lite]** A Closer Look at Self-Supervised Lightweight Vision Transformers | **[ICML'23]** | [`[paper]`](https://arxiv.org/abs/2205.14443) [`[code]`](https://github.com/wangsr126/mae-lite)

- **[SupMAE]** SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14540) [`[code]`](https://github.com/enyac-group/supmae)

- **[HiViT]** HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2205.14949) [`[mmpretrian code]`](https://github.com/open-mmlab/mmpretrain)

- **[LoMaR]** Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2206.00790) [`[code]`](https://github.com/junchen14/LoMaR)

- **[SIM]** Siamese Image Modeling for Self-Supervised Vision Representation Learning | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2206.01204) [`[code]`](https://github.com/OpenGVLab/Siamese-Image-Modeling)

- **[MFM]** Masked Frequency Modeling for Self-Supervised Visual Pre-Training | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2206.07706) [`[code]`](https://github.com/Jiahao000/MFM)

- **[BootMAE]** Bootstrapped Masked Autoencoders for Vision BERT Pretraining | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2207.07116) [`[code]`](https://github.com/LightDXY/BootMAE)

- **[CMAE]** Contrastive Masked Autoencoders are Stronger Vision Learners | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2207.13532) [`[code]`](https://github.com/ZhichengHuang/CMAE)

- **[SMD]** Improving Self-supervised Lightweight Model Learning via Hard-aware Metric Distillation | **[ECCV'22]** | [`[paper]`](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910286.pdf) [`[code]`](https://github.com/liuhao-lh/SMD/tree/main)

- **[SdAE]** SdAE: Self-distillated Masked Autoencoder | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2208.00449) [`[code]`](https://github.com/AbrahamYabo/SdAE)

- **[MILAN]** MILAN: Masked Image Pretraining on Language Assisted Representation | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2208.06049) [`[code]`](https://github.com/zejiangh/MILAN)

- **[BEiTv2]** BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2208.06366) [`[code]`](https://github.com/microsoft/unilm)

- **[BEiTv3]** Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2208.10442) [`[code]`](https://github.com/microsoft/unilm)

- **[MaskCLIP]** MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2208.12262) [`[code]`](https://github.com/LightDXY/MaskCLIP)

- **[MimCo]** MimCo: Masked Image Modeling Pre-training with Contrastive Teacher | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2209.03063)

- **[VICRegL]** VICRegL: Self-Supervised Learning of Local Visual Features | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2210.01571) [`[code]`](https://github.com/facebookresearch/VICRegL)

- **[SSLight]** Effective Self-supervised Pre-training on Low-compute Networks without Distillation | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2210.02808) [`[code]`](https://github.com/saic-fi/SSLight)

- **[U-MAE]** How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2210.08344) [`[code]`](https://github.com/zhangq327/U-MAE)

- **[i-MAE]** i-MAE: Are Latent Representations in Masked Autoencoders Linearly Separable? | **[axiv'22]** | [`[paper]`](https://arxiv.org/abs/2210.11470) [`[code]`](https://github.com/VILA-Lab/i-mae)

- **[CAN]** A simple, efficient and scalable contrastive masked autoencoder for learning visual representations | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2210.16870) [`[code]`](https://github.com/shlokk/mae-contrastive)

- **[EVA]** EVA: Exploring the Limits of Masked Visual Representation Learning at Scale | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2211.07636) [`[code]`](https://github.com/baaivision/EVA)

- **[CAEv2]** CAE v2: Context Autoencoder with CLIP Target | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2211.09799)

- **[iTPN]** Integrally Pre-Trained Transformer Pyramid Networks | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2211.12735) [`[code]`](https://github.com/sunsmarterjie/iTPN)

- **[SCFS]** Semantics-Consistent Feature Search for Self-Supervised Visual Representation Learning | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2212.06486) [`[code]`](https://github.com/skyoux/scfs)

- **[FastMIM]** FastMIM: Expediting Masked Image Modeling Pre-training for Vision | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2212.06593) [`[code]`](https://github.com/ggjy/FastMIM.pytorch)

- **[Light-MoCo]** Establishing a stronger baseline for lightweight contrastive models | **[ICME'23]** | [`[paper]`](https://arxiv.org/abs/2212.07158)  [`[code]`](https://github.com/Linwenye/light-moco) [`[ICLR'23 under-review version]`](https://openreview.net/pdf?id=9CGiwZeCAd)

- **[Scale-MAE]** Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2212.14532)

## 2023

- **[ConvNeXtv2]** ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2301.00808) [`[code]`](https://github.com/facebookresearch/ConvNeXt-V2)

- **[Spark]** Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2301.03580) [`[code]`](https://github.com/keyu-tian/SparK)

- **[I-JEPA]** Self-SupervisedÂ LearningÂ fromÂ ImagesÂ with aÂ Joint-EmbeddingÂ PredictiveÂ Architecture | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2301.08243) [`[code]`](https://github.com/facebookresearch/ijepa)

- **[RoB]** A Simple Recipe for Competitive Low-compute Self supervised Vision Models | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2301.09451)

- **[Layer Grafted]** Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2302.14138) [`[code]`](https://github.com/VITA-Group/layerGraftedPretraining_ICLR23)

- **[PixMIM]** PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2303.02416) [`[code]`](https://github.com/open-mmlab/mmselfsup)

- **[LocalMIM]** Masked Image Modeling with Local Multi-Scale Reconstruction | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2303.05251) [`[code]`](https://github.com/huawei-noah/Efficient-Computing/tree/master/Self-supervised/LocalMIM)
 
- **[MR-MAE]** Mimic before Reconstruct: Enhancing Masked Autoencoders with Feature Mimicking | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2303.05475)

- **[MixedAE]** Mixed Autoencoder for Self-supervised Visual Representation Learning | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2303.17152)

- **[EMP]** EMP-SSL: Towards Self-Supervised Learning in One Training Epoch | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2304.03977) [`[code]`](https://github.com/tsb0601/EMP-SSL)

- **[DINOv2]** DINOv2ï¼šLearning Robust Visual Features without Supervision | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2304.07193) [`[code]`](https://github.com/facebookresearch/dinov2)

- **[CL-vs-MIM]** What Do Self-Supervised Vision Transformers Learn? | **[ICLR'23]** |[`[paper]`](https://arxiv.org/abs/2305.00729) [`[code]`](https://github.com/naver-ai/cl-vs-mim)

- **[SiamMAE]** Siamese Masked Autoencoders | **[NIPS'23]** | [`[paper]`](https://arxiv.org/abs/2305.14344)

- **[ccMIM]** Contextual Image Masking Modeling via Synergized Contrasting without View Augmentation for Faster and Better Visual Pretraining | **[ICLR'23]** | [`[paper]`](https://openreview.net/pdf?id=A3sgyt4HWp) [`[code]`](https://github.com/Sherrylone/ccMIM)

- **[DreamTeacher]** DreamTeacher: Pretraining Image Backbones with Deep Generative Models | **[ICCV'23]** |[`[paper]`](https://arxiv.org/abs/2307.07487)

- **[MFF]** Improving Pixel-based MIM by Reducing Wasted Modeling Capability | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2308.00261) [`[code]`](https://github.com/open-mmlab/mmpretrain)

- **[DropPos]** DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions | **[NIPS'23]** | [`[paper]`](https://arxiv.org/abs/2309.03576) [`[code]`](https://github.com/Haochen-Wang409/DropPos)
