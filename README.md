# Self-Supervised Pretraining
This is a paper list of self-supervised pretraining method. All papers are listed in order of their appearance in arxiv. 

In addition, papers are also categorized according to different topics. You can click on the links below to get related papers on the topics you are interested in.

- [Contrastive Learning/Joint Embedding](src/Contrastive_Learning.md)
- [Masked Image Modeling](src/Masked_Image_Modeling.md)
- [Light-weight Model Pretraing](src/Light-weight_Model_Pretraining.md)
- [CNN Pretraing]()
- [ViT Pretraining]()
- [Hierarchical Model Pretraining]()
- [Dense Prediction Model Pretraining]()
- [Large Vision Model/Foundation Model Pretraining]()
- updating Â·Â·Â·


# All Papers

## 2020

- **[MoCov1]** ðŸŒŸ Momentum Contrast for Unsupervised Visual Representation Learningn | **[CVPR'20]** |[`[paper]`](https://arxiv.org/abs/1911.05722) [`[code]`](https://github.com/facebookresearch/moco) 
   <details close>
   <summary>MoCov1 Arch</summary>
   <p align="center"><img width="50%" src="https://user-images.githubusercontent.com/66102178/278521451-0db281de-9c7c-4292-8e52-2fe125fb4afa.png" /></p>
   </details>

- **[SimCLRv1]** ðŸŒŸ A Simple Framework for Contrastive Learning of Visual Representations | **[ICML'20]** | [`[paper]`](https://arxiv.org/abs/2002.05709) [`[code]`](https://github.com/google-research/simclr)
   <details close>
   <summary>SimCLRv1 Arch</summary>
   <p align="center"><img width="50%" src="https://user-images.githubusercontent.com/66102178/278521501-9b04ab1f-e359-4a52-8b0a-c3a794bac1cb.png" /></p>
   </details>
- How Useful is Self-Supervised Pretraining for Visual Tasks? | **[CVPR'20]** | [`[paper]`](https://arxiv.org/abs/2003.14323)

- **[MoCov2]** Improved Baselines with Momentum Contrastive Learning | **[arxiv'20]** | [`[paper]`](https://arxiv.org/abs/2003.04297) [`[code]`](https://github.com/facebookresearch/moco)
   <details close>
   <summary>MoCov2 Arch</summary>
   <p align="center"><img width="50%" src="https://user-images.githubusercontent.com/66102178/278521477-0f1bacd5-6a46-4c5f-bfac-de6f8ec8898d.png" /></p>
   </details>

- **[BYOL]** Bootstrap your own latent: A new approach to self-supervised Learning | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.07733) [`[code]`](https://github.com/deepmind/deepmind-research)
   <details close>
   <summary>BYOL Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278522794-990ed1bf-4e4c-4f2b-8346-225ac84b325a.png" /></p>
   </details>

- **[SimCLRv2]** Big Self-Supervised Models are Strong Semi-Supervised Learners | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.10029) [`[code]`](https://github.com/google-research/simclr)
   <details close>
   <summary>SimCLRv2 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278523316-ae6fc7b3-3908-4373-8fe2-21ec808fa9a0.png" /></p>
   </details>

- **[SwAV]** Unsupervised Learning of Visual Features by Contrasting Cluster Assignments | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.09882) [`[code]`](https://github.com/facebookresearch/swav)
   <details close>
   <summary>SwAV Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278523332-c1f8a4fb-2c1a-4d93-8465-de449e9be23e.png" /></p>
   </details>

- **[RELICv1]** Representation Learning via Invariant Causal Mechanisms | **[ICLR'21]** | [`[paper]`](https://arxiv.org/abs/2010.07922)

- **[CompRess]** CompRess: Self-Supervised Learning by Compressing Representations | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2010.14713) [`[code]`](https://github.com/UMBCvision/CompReSS)
   <details close>
   <summary>CompRess Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278523953-090209e4-5af6-4fd8-9b3a-45552fec3fef.png" /></p>
   </details>

- **[DenseCL]** Dense Contrastive Learning for Self-Supervised Visual Pre-Training | **[CVPR'21]** | [`[paper]`](https://arxiv.org/abs/2011.09157) [`[code]`](https://github.com/WXinlong/DenseCL)
   <details close>
   <summary>DenseCL Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278523967-c201aa64-d163-43a2-b5e7-a0f43d81b083.png" /></p>
   </details>

- **[SimSiam]** ðŸŒŸ Exploring Simple Siamese Representation Learning | **[CVPR'21]** [`[paper]`](https://arxiv.org/abs/2011.10566) [`[code]`](https://github.com/facebookresearch/simsiam)
   <details close>
   <summary>SimSiam Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/278523977-fa990e64-08af-44d6-a7ce-cbdf5766bd0b.png" /></p>
   </details>

## 2021

- **[SEED]** SEED: Self-supervised Distillation For Visual Representation | **[ICLR'21]** | [`[paper]`](https://arxiv.org/abs/2101.04731) [`[code]`](https://github.com/zhangyifei01/SEED_ICLR21)
   <details close>
   <summary>SEED Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278526068-55cb1d88-2d59-44a3-a998-a2238886f4db.png" /></p>
   </details>

- **[ALIGN]** Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision | **[ICML'21]** | [`[paper]`](https://arxiv.org/abs/2102.05918)
   <details close>
   <summary>ALIGN Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/281976273-b7d063e9-1c52-4f51-8988-5da692984f8e.png" /></p>
   </details>

- **[CLIP]** ðŸŒŸ Learning Transferable Visual Models From Natural Language Supervision | **[ICML'21]** | [`[paper]`](https://arxiv.org/abs/2103.00020) [`[code]`](https://github.com/OpenAI/CLIP)
   <details close>
   <summary>CLIP Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527705-9e2961d5-76d3-4b55-8af1-dc52854124b4.png" /></p>
   </details>

- **[Barlow Twins]** Barlow Twins: Self-Supervised Learning via Redundancy Reduction | **[ICML'21]** | [`[paper]`](https://arxiv.org/abs/2103.03230) [`[code]`](https://github.com/facebookresearch/barlowtwins)
   <details close>
   <summary>Barlow Twins Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527722-e8974a70-a503-4413-8a74-550c4af73e76.png" /></p>
   </details>

- **[S3L]** Rethinking Self-Supervised Learning: Small is Beautiful | **[arxiv'21]** | [`[paper]`](https://arxiv.org/abs/2103.13559) [`[code]`](https://github.com/CupidJay/Scaled-down-self-supervised-learning)

- **[MoCov3]** ðŸŒŸ An Empirical Study of Training Self-Supervised Vision Transformers | **[ICCV'21]** | [`[paper]`](https://arxiv.org/abs/2104.02057) [`[code]`](https://github.com/facebookresearch/moco-v3)

- **[DisCo]** DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2104.09124) [`[code]`](https://github.com/Yuting-Gao/DisCo-pytorch)
   <details close>
   <summary>DisCo Arch</summary>
   <p align="center"><img width="80%" src="https://user-images.githubusercontent.com/66102178/278527927-36c1a899-140d-406e-bb26-251dd32fe3e4.png" /></p>
   </details>

- **[DoGo]** Distill on the Go: Online knowledge distillation in self-supervised learning | **[CVPRW'21]** | [`[paper]`](https://arxiv.org/abs/2104.09866) [`[code]`](https://github.com/NeurAI-Lab/DoGo)
   <details close>
   <summary>DoGo Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278527946-df193c6f-f4e0-4372-b0c1-b8e9af397cc2.png" /></p>
   </details>

- **[DINOv1]** ðŸŒŸ Emerging Properties in Self-Supervised Vision Transformers | **[ICCV'21]** |[`[paper]`](https://arxiv.org/abs/2104.14294) [`[code]`](https://github.com/facebookresearch/dino)
   <details close>
   <summary>DINOv1 Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/278527957-77176a63-50ae-4048-b939-33b1ff76235c.png" /></p>
   </details>

- **[VICReg]** VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2105.04906) [`[code]`](https://github.com/facebookresearch/vicreg)
   <details close>
   <summary>VICReg Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278527974-00fa13fc-fca9-4922-a56a-b25e325bf4bf.png" /></p>
   </details>

- **[MST]** MST: Masked Self-Supervised Transformer for Visual Representation | **[NIPS'21]** | [`[paper]`](https://arxiv.org/abs/2106.05656) 
   <details close>
   <summary>MST Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278527994-ef413def-2a1d-49bf-ba71-37b64f860155.png" /></p>
   </details>

- **[BEiTv1]** ðŸŒŸ BEiT: BERT Pre-Training of Image Transformers | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2106.08254) [`[code]`](https://github.com/microsoft/unilm)
   <details close>
   <summary>BEiTv1 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278530019-f87113af-3180-4e51-9801-57d8ec426c0c.png" /></p>
   </details>

- **[SimDis]** Simple Distillation Baselines for Improving Small Self-supervised Models | **[ICCVW'21]** | [`[paper]`](https://arxiv.org/abs/2106.11304) [`[code]`](https://github.com/JindongGu/SimDis)
   <details close>
   <summary>SimDis Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278535661-b303cfbe-7a43-4fe3-88cc-babee0a52346.png" /></p>
   </details>

- **[OSS]** Unsupervised Representation Transfer for Small Networks: I Believe I Can Distill On-the-Fly | **[NIPS'21]** | [`[paper]`](https://proceedings.neurips.cc/paper_files/paper/2021/hash/cecd845e3577efdaaf24eea03af4c033-Abstract.html)
   <details close>
   <summary>OSS Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278535677-e0169f0f-4461-4e9f-bf96-fa72246a2f4f.png" /></p>
   </details>

- **[BINGO]** Bag of Instances Aggregation Boosts Self-supervised Distillation | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2107.01691) [`[code]`](https://github.com/haohang96/bingo)
   <details close>
   <summary>BINGO Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278535686-1f9b8ec7-66ac-4735-aad2-6adaa7615f51.png" /></p>
   </details>

- **[SSL-Small]** On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals | **[AAAI'22]** | [`[paper]`](https://arxiv.org/abs/2107.14762) [`[code]`](https://github.com/WOWNICE/ssl-small)

- **[C-BYOL/C-SimLCR]** Compressive Visual Representations | **[NIPS'21]** | [`[paper]`](https://arxiv.org/abs/2109.12909) [`[code]`](https://github.com/google-research/compressive-visual-representations)

- **[MAE]** ðŸŒŸ Masked Autoencoders Are Scalable Vision Learners | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2111.06377) [`[code]`](https://github.com/facebookresearch/mae)
   <details close>
   <summary>MAE Arch</summary>
   <p align="center"><img width="80%" src="https://user-images.githubusercontent.com/66102178/278535696-62557901-d082-4f49-9a24-10d81e06c649.png" /></p>
   </details>

- **[iBOT]** iBOT: Image BERT Pre-Training with Online Tokenizer | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2111.07832) [`[code]`](https://github.com/bytedance/ibot)
   <details close>
   <summary>iBOT Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278535707-7d10a773-5ca6-4cb7-9e57-99191fa0617f.png" /></p>
   </details>

- **[SimMIM]** ðŸŒŸ SimMIM: A Simple Framework for Masked Image Modeling | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2111.09886) [`[code]`](https://github.com/microsoft/SimMIM)
   <details close>
   <summary>SimMIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278535722-70041e7f-3da3-4852-8780-f932903c615e.png" /></p>
   </details>

- **[PeCo]** PeCoï¼šPerceptual Codebook for BERT Pre-training of Vision Transformers | **[AAAI'23]** | [`[paper]`](https://arxiv.org/abs/2111.12710)
   <details close>
   <summary>PeCo Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278535737-2f090bd3-5589-4ffb-8c97-31353ac707e9.png" /></p>
   </details>

- **[MaskFeat]** Masked Feature Prediction for Self-Supervised Visual Pre-Training | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2112.09133) [`[code]`](https://github.com/facebookresearch/SlowFast)
   <details close>
   <summary>MaskFeat Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278535752-bd201a87-1687-4e2e-b2d6-d45fdb14ec13.png" /></p>
   </details>

## 2022

- **[RELICv2]** Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet? | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2201.05119)
   <details close>
   <summary>RELICv2 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278783597-b0ae0798-96d3-4e88-9d8c-73cbdefa5f30.png" /></p>
   </details>

- **[SimReg]** SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation | **[BMVC'21]** | [`[paper]`](https://arxiv.org/abs/2201.05131) [`[code]`](https://github.com/UCDvision/simreg)
   <details close>
   <summary>SimReg Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278783628-166af556-d4e4-4db6-929c-bb4638bd8c2e.png" /></p>
   </details>

- **[RePre]** RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2201.06857)
   <details close>
   <summary>RePre Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783635-70fe848a-8613-44d2-ad25-7fddcd5ebc86.png" /></p>
   </details>

- **[CAEv1]** Context Autoencoder for Self-Supervised Representation Learning | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2202.03026) [`[code]`](https://github.com/lxtGH/CAE)
   <details close>
   <summary>CAEv1 Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783643-8b4d11fa-acd7-4d71-b02c-075f1917f8c1.png" /></p>
   </details>

- **[CIM]** Corrupted Image Modeling for Self-Supervised Visual Pre-Training | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2202.03382)
   <details close>
   <summary>CIM Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783648-c93a3c22-b10f-4a53-a89d-25e4a52695da.png" /></p>
   </details>

- **[MVP]** MVP: Multimodality-guided Visual Pre-training | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2203.05175)
   <details close>
   <summary>MVP Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783650-1915f976-cd80-4dba-a8b4-4bc357dace1e.png" /></p>
   </details>

- **[ConvMAE]** ConvMAE: Masked Convolution Meets Masked Autoencoders | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2205.03892) [`[code]`](https://github.com/Alpha-VL/ConvMAE)
   <details close>
   <summary>ConvMAE Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783657-82d3f0a5-4f5d-4767-a720-9b51d6b2704d.png" /></p>
   </details>

- **[ConMIM]** Masked Image Modeling with Denoising Contrast | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2205.09616) [`[code]`](https://github.com/TencentARC/ConMIM)
   <details close>
   <summary>ConMIM Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/278783673-1a3be697-056e-4346-a204-a56904ef525e.png" /></p>
   </details>

- **[MixMAE]** MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers | **[CVPR'23]** |[`[paper]`](https://arxiv.org/abs/2205.13137) [`[code]`](https://github.com/Sense-X/MixMIM)
   <details close>
   <summary>MixMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/278783687-7c70612e-8d22-44f1-a58e-2fbc3b74094e.png" /></p>
   </details>

- **[A2MIM]** Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN | **[ICML'23]** | [`[paper]`](https://arxiv.org/abs/2205.13943) [`[code]`](https://github.com/Westlake-AI/A2MIM)
   <details close>
   <summary>A2MIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017037-cfbc32c1-ddf6-4436-a2aa-3521106616a1.png" /></p>
   </details>

- **[FD]** Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14141) [`[code]`](https://github.com/SwinTransformer/Feature-Distillation)
   <details close>
   <summary>FD Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/279017061-3e545bb4-aef7-4eb8-93a2-6e22e6bb1668.png" /></p>
   </details>

- **[ObjMAE]** Object-wise Masked Autoencoders for Fast Pre-training | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14338)
   <details close>
   <summary>ObjMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017077-f987df1f-7607-4fa6-bebc-7253829c058e.png" /></p>
   </details>

- **[MAE-Lite]** A Closer Look at Self-Supervised Lightweight Vision Transformers | **[ICML'23]** | [`[paper]`](https://arxiv.org/abs/2205.14443) [`[code]`](https://github.com/wangsr126/mae-lite)
   <details close>
   <summary>MAE-Lite Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017113-8b961115-23d0-438c-b0e0-ffcbd639ec53.png" /></p>
   </details>

- **[SupMAE]** SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14540) [`[code]`](https://github.com/enyac-group/supmae)
   <details close>
   <summary>SupMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017126-3784511f-7583-4c09-9610-90775877a9d9.png" /></p>
   </details>

- **[HiViT]** HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2205.14949) [`[mmpretrian code]`](https://github.com/open-mmlab/mmpretrain)
   <details close>
   <summary>HiViT Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017144-18dbf120-f8a7-4149-b651-3fedc3269122.png" /></p>
   </details>

- **[LoMaR]** Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2206.00790) [`[code]`](https://github.com/junchen14/LoMaR)
   <details close>
   <summary>LoMaR Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017165-7854db74-446b-42c6-96d7-f2d89b8e8ccc.png" /></p>
   </details>

- **[SIM]** Siamese Image Modeling for Self-Supervised Vision Representation Learning | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2206.01204) [`[code]`](https://github.com/OpenGVLab/Siamese-Image-Modeling)
   <details close>
   <summary>SIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017191-9689d5b7-aab8-4e9f-99b9-386bcdcf46e5.png" /></p>
   </details>

- **[MFM]** Masked Frequency Modeling for Self-Supervised Visual Pre-Training | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2206.07706) [`[code]`](https://github.com/Jiahao000/MFM)
   <details close>
   <summary>MFM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017228-86469272-c029-468b-ab0e-fb589bafd3bc.png" /></p>
   </details>

- **[BootMAE]** Bootstrapped Masked Autoencoders for Vision BERT Pretraining | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2207.07116) [`[code]`](https://github.com/LightDXY/BootMAE)
   <details close>
   <summary>BootMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017260-45a0d61c-7d40-4a83-9849-34ed8d1cddd8.png" /></p>
   </details>

- **[SatMAE]** SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2207.08051) [`[code]`](https://github.com/sustainlab-group/SatMAE)

- **[TinyViT]** TinyViT: Fast Pretraining Distillation for Small Vision Transformers | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2207.10666) [`[code]`](https://github.com/microsoft/Cream/tree/main/TinyViT)

- **[CMAE]** Contrastive Masked Autoencoders are Stronger Vision Learners | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2207.13532) [`[code]`](https://github.com/ZhichengHuang/CMAE)
   <details close>
   <summary>CMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017308-51b1ffeb-eaec-4b30-bb7a-2bed94a5f6f3.png" /></p>
   </details>

- **[SMD]** Improving Self-supervised Lightweight Model Learning via Hard-aware Metric Distillation | **[ECCV'22]** | [`[paper]`](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910286.pdf) [`[code]`](https://github.com/liuhao-lh/SMD/tree/main)
   <details close>
   <summary>SMD Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017350-2fd6329b-7131-41f8-a6d8-8a7e0ac6bda2.png" /></p>
   </details>

- **[SdAE]** SdAE: Self-distillated Masked Autoencoder | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2208.00449) [`[code]`](https://github.com/AbrahamYabo/SdAE)
   <details close>
   <summary>SdAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017380-961241a1-af11-4962-8528-383a9c6f2b04.png" /></p>
   </details>

- **[MILAN]** MILAN: Masked Image Pretraining on Language Assisted Representation | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2208.06049) [`[code]`](https://github.com/zejiangh/MILAN)
   <details close>
   <summary>MILAN Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017407-2975d5f9-aa9f-4e74-b060-706c351d9147.png" /></p>
   </details>

- **[BEiTv2]** BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2208.06366) [`[code]`](https://github.com/microsoft/unilm)
   <details close>
   <summary>BEiTv2 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017794-e9976a9d-f85c-4167-b425-12b8c3a5d845.png" /></p>
    <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017814-8b4aab8f-19ba-44b9-a714-6ddbb8de7668.png" /></p>
   </details>

- **[BEiTv3]** Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2208.10442) [`[code]`](https://github.com/microsoft/unilm)
   <details close>
   <summary>BEiTv3 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017449-107f3b81-264b-4af5-aae1-e3444e33a436.png" /></p>
   </details>

- **[MaskCLIP]** MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2208.12262) [`[code]`](https://github.com/LightDXY/MaskCLIP)
   <details close>
   <summary>BEiTv3 Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017487-ac4c4cef-fd3e-423f-bc91-0d4e0c6e1925.png" /></p>
   </details>

- **[MimCo]** MimCo: Masked Image Modeling Pre-training with Contrastive Teacher | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2209.03063)
   <details close>
   <summary>MimCo Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017531-451d29ec-0e8f-490f-9f8d-68908fe8671d.png" /></p>
   </details>

- **[VICRegL]** VICRegL: Self-Supervised Learning of Local Visual Features | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2210.01571) [`[code]`](https://github.com/facebookresearch/VICRegL)
   <details close>
   <summary>VICRegL Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279017555-c853ba70-b6b5-4f42-8df7-b6de044921b9.png" /></p>
   </details>

- **[SSLight]** Effective Self-supervised Pre-training on Low-compute Networks without Distillation | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2210.02808) [`[code]`](https://github.com/saic-fi/SSLight)

- **[U-MAE]** How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2210.08344) [`[code]`](https://github.com/zhangq327/U-MAE)

- **[i-MAE]** i-MAE: Are Latent Representations in Masked Autoencoders Linearly Separable? | **[axiv'22]** | [`[paper]`](https://arxiv.org/abs/2210.11470) [`[code]`](https://github.com/VILA-Lab/i-mae)
   <details close>
   <summary>i-MAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293573-650c324f-5580-42e6-857f-33a9fde517e6.png" /></p>
   </details>

- **[CAN]** A simple, efficient and scalable contrastive masked autoencoder for learning visual representations | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2210.16870) [`[code]`](https://github.com/shlokk/mae-contrastive)
   <details close>
   <summary>CAN Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293610-efa98556-8cfd-4184-908b-af47ad88f01d.png" /></p>
   </details>

- **[EVA]** EVA: Exploring the Limits of Masked Visual Representation Learning at Scale | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2211.07636) [`[code]`](https://github.com/baaivision/EVA)
   <details close>
   <summary>EVA Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/279293632-2b2397ac-ab5c-4c7d-8896-7200bb15d95b.png" /></p>
   </details>

- **[CAEv2]** CAE v2: Context Autoencoder with CLIP Target | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2211.09799)
   <details close>
   <summary>CAEv2 Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/279293651-6ca73d8c-d262-404d-93f4-5678918ddc3d.png" /></p>
   </details>

- **[iTPN]** Integrally Pre-Trained Transformer Pyramid Networks | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2211.12735) [`[code]`](https://github.com/sunsmarterjie/iTPN)
   <details close>
   <summary>iTPN Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293671-2ba443b3-5d5e-4b78-92af-ba544c7f6b6a.png" /></p>
   </details>

- **[SCFS]** Semantics-Consistent Feature Search for Self-Supervised Visual Representation Learning | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2212.06486) [`[code]`](https://github.com/skyoux/scfs)
   <details close>
   <summary>SCFS Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293684-8f45ed8d-153a-48e6-b328-3cf80131bbdc.png" /></p>
   </details>

- **[FastMIM]** FastMIM: Expediting Masked Image Modeling Pre-training for Vision | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2212.06593) [`[code]`](https://github.com/ggjy/FastMIM.pytorch)
   <details close>
   <summary>FastMIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293703-3b6ccccf-b150-49b3-b8b8-c8977491cff7.png" /></p>
   </details>

- **[Light-MoCo]** Establishing a stronger baseline for lightweight contrastive models | **[ICME'23]** | [`[paper]`](https://arxiv.org/abs/2212.07158)  [`[code]`](https://github.com/Linwenye/light-moco) [`[ICLR'23 under-review version]`](https://openreview.net/pdf?id=9CGiwZeCAd)
   <details close>
   <summary>Light-MoCo Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293730-035ea435-966a-4396-a73c-9a2e4cdeadd3.png" /></p>
   </details>

- **[Scale-MAE]** Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2212.14532)
   <details close>
   <summary>Scale-MAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293746-3ef633f8-2878-4314-b14e-15fb6eac499e.png" /></p>
   </details>

## 2023

- **[ConvNeXtv2]** ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2301.00808) [`[code]`](https://github.com/facebookresearch/ConvNeXt-V2)
   <details close>
   <summary>ConvNeXtv2 Arch</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/66102178/279293774-0b04c28e-8852-4704-85fd-428f58e570ed.png" /></p>
   </details>

- **[Spark]** Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2301.03580) [`[code]`](https://github.com/keyu-tian/SparK)
   <details close>
   <summary>Spark Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293789-7f106316-0635-42f6-b67f-309652b32903.png" /></p>
   </details>

- **[I-JEPA]** Self-SupervisedÂ LearningÂ fromÂ ImagesÂ with aÂ Joint-EmbeddingÂ PredictiveÂ Architecture | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2301.08243) [`[code]`](https://github.com/facebookresearch/ijepa)
   <details close>
   <summary>I-JEPA Arch</summary>
   <p align="center"><img width="70%" src="https://user-images.githubusercontent.com/66102178/279293807-e1f5fdbc-ba14-4902-b70a-a9c577dbb7e9.png" /></p>
   </details>

- **[RoB]** A Simple Recipe for Competitive Low-compute Self supervised Vision Models | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2301.09451)
   <details close>
   <summary>RoB Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293823-b9594220-366b-4645-bf5e-c1a95f81410a.png" /></p>
   </details>

- **[Layer Grafted]** Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2302.14138) [`[code]`](https://github.com/VITA-Group/layerGraftedPretraining_ICLR23)
   <details close>
   <summary>Layer Grafted Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293841-f7f38afa-6121-408c-8d43-a5e1c3f5c10c.png" /></p>
   </details>

- **[G2SD]** Generic-to-Specific Distillation of Masked Autoencoders | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2302.14771) [`[code]`](https://github.com/pengzhiliang/G2SD)
   <details close>
   <summary>G2SD Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/284453784-26498350-f7fa-4601-8b49-c9455e3f936f.png" /></p>
   </details>

- **[PixMIM]** PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2303.02416) [`[code]`](https://github.com/open-mmlab/mmselfsup)
   <details close>
   <summary>PixMIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293855-868da4c3-ad67-4c36-8df0-e7ebb88fb075.png" /></p>
   </details>

- **[LocalMIM]** Masked Image Modeling with Local Multi-Scale Reconstruction | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2303.05251) [`[code]`](https://github.com/huawei-noah/Efficient-Computing/tree/master/Self-supervised/LocalMIM)
   <details close>
   <summary>LocalMIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293880-03bbb206-9c7f-42a6-80b0-c19504dc07ad.png" /></p>
   </details>
 
- **[MR-MAE]** Mimic before Reconstruct: Enhancing Masked Autoencoders with Feature Mimicking | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2303.05475)
   <details close>
   <summary>MR-MAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293915-81d80e37-7e57-4bac-889b-e0d1020fc577.png" /></p>
   </details>

- **[Overcoming-Pretraining-Bias]** Overwriting Pretrained Bias with Finetuning Data | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2303.05475) [`[code]`](https://github.com/princetonvisualai/overcoming-pretraining-bias)
   <details close>
   <summary>Overcoming-Pretraining-Bias Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/285569041-4b8d86b7-cab7-4381-9bef-8a3560280f18.png" /></p>
   </details>

- **[EVA-02]** EVA-02: A Visual Representation for Neon Genesis | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2303.11331) [`[code]`](https://github.com/baaivision/EVA)

- **[EVA-CLIP]** EVA-CLIP: Improved Training Techniques for CLIP at Scale | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2303.15389) [`[code]`](https://github.com/baaivision/EVA)
  
- **[MixedAE]** Mixed Autoencoder for Self-supervised Visual Representation Learning | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2303.17152)
   <details close>
   <summary>MixedAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293942-ae68ace0-725a-4b4e-a494-6321dbe8dd30.png" /></p>
   </details>

- **[EMP]** EMP-SSL: Towards Self-Supervised Learning in One Training Epoch | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2304.03977) [`[code]`](https://github.com/tsb0601/EMP-SSL)
   <details close>
   <summary>EMP Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293962-94c6168d-4e60-4883-b9b7-f9e2e662863a.png" /></p>
   </details>

- **[DINOv2]** DINOv2ï¼šLearning Robust Visual Features without Supervision | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2304.07193) [`[code]`](https://github.com/facebookresearch/dinov2)

- **[CL-vs-MIM]** What Do Self-Supervised Vision Transformers Learn? | **[ICLR'23]** |[`[paper]`](https://arxiv.org/abs/2305.00729) [`[code]`](https://github.com/naver-ai/cl-vs-mim)

- **[SiamMAE]** Siamese Masked Autoencoders | **[NIPS'23]** | [`[paper]`](https://arxiv.org/abs/2305.14344)
   <details close>
   <summary>SiamMAE Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279293985-7c58e776-de93-440d-878c-f3f596dfea5a.png" /></p>
   </details>

- **[ccMIM]** Contextual Image Masking Modeling via Synergized Contrasting without View Augmentation for Faster and Better Visual Pretraining | **[ICLR'23]** | [`[paper]`](https://openreview.net/pdf?id=A3sgyt4HWp) [`[code]`](https://github.com/Sherrylone/ccMIM)
   <details close>
   <summary>ccMIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279294010-0d90109e-bab9-4377-9848-19efcdf73f8a.png" /></p>
   </details>

- **[MaskSub]** Masking meets Supervision: A Strong Learning Alliance | **[CVPR'25]** |[`[paper]`](https://arxiv.org/pdf/2306.11339) [`[code]`](https://github.com/naver-ai/augsub)

- **[DreamTeacher]** DreamTeacher: Pretraining Image Backbones with Deep Generative Models | **[ICCV'23]** |[`[paper]`](https://arxiv.org/abs/2307.07487)
   <details close>
   <summary>DreamTeacher Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279294020-e461e141-1ccb-471f-9520-bf880c615843.png" /></p>
   </details>

- **[MFF]** Improving Pixel-based MIM by Reducing Wasted Modeling Capability | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2308.00261) [`[code]`](https://github.com/open-mmlab/mmpretrain)
   <details close>
   <summary>MFF Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279294038-118cb80c-2479-4b7e-a6b0-6e44e7d745a6.png" /></p>
   </details>

- **[DropPos]** DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions | **[NIPS'23]** | [`[paper]`](https://arxiv.org/abs/2309.03576) [`[code]`](https://github.com/Haochen-Wang409/DropPos)
   <details close>
   <summary>DropPos Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/279294057-a4f7f653-18fa-4d44-ad27-f30eb4d5e02c.png" /></p>
   </details>

- **[Registers]** Vision Transformers Need Registers | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2309.16588) [`[code]`](https://github.com/facebookresearch/dinov2)
   <details close>
   <summary>Registers Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/285569143-e909c808-3a68-449a-bd22-8ea17f942384.png" /></p>
   </details>

- **[MetaCLIP]** Demystifying CLIP Data | **[ICLR'24]** | [`[paper]`](https://arxiv.org/pdf/2309.16671) [`[code]`](https://github.com/facebookresearch/MetaCLIP)

- **[AMD]** Asymmetric Masked Distillation for Pre-Training Small Foundation Models | **[CVPR'24]** | [`[paper]`](https://arxiv.org/abs/2311.03149) [`[code]`](https://github.com/MCG-NJU/AMD)

- **[D-iGPT]** Rejuvenating image-GPT as Strong Visual Representation Learners | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2312.02147) [`[code]`](https://github.com/OliverRensu/D-iGPT)
   <details close>
   <summary>D-iGPT Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/287959167-c5c3b2a4-2868-4461-a8ef-af1bef4daa34.png" /></p>
   </details>

- **[SynCLR]**  Learning Vision from Models Rivals Learning Vision from Data | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2312.17742) [`[code]`](https://github.com/google-research/syn-rep-learn)

## 2024
- **[AIM]** Scalable Pre-training of Large Autoregressive Image Models | **[arxiv'24]** | [`[paper]`](https://arxiv.org/abs/2401.08541) [`[code]`](https://github.com/apple/ml-aim)
   <details close>
   <summary>AIM Arch</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/66102178/297312612-f5aa4d62-9930-4967-955f-5d84a268a63c.png" /></p>
   </details>

- **[CrossMAE]** Rethinking Patch Dependence for Masked Autoencoders | **[arxiv'24]** | [`[paper]`](https://arxiv.org/abs/2401.14391) [`[code]`](https://github.com/TonyLianLong/CrossMAE)
   <details close>
   <summary>CrossMAE Arch</summary>
   <p align="center"><img width="90%" src="https://private-user-images.githubusercontent.com/66102178/301049334-d5648ac5-37cc-4cc3-9751-4a94a70a3da1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDY2Nzk3MjgsIm5iZiI6MTcwNjY3OTQyOCwicGF0aCI6Ii82NjEwMjE3OC8zMDEwNDkzMzQtZDU2NDhhYzUtMzdjYy00Y2MzLTk3NTEtNGE5NGE3MGEzZGExLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAxMzElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMTMxVDA1MzcwOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBiMDI3MWZjMGYzMTMxZjI4YzY3NmVmMDQxNzVmNjM2MjRhMTgwZDUxMWUxM2NmZTliYmUwNzkwNWI5ZjY1YWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.84_qSjzz7LK6nadov0UgYLv3HtGkKrV-BpLpOMkQu0E" /></p>
   </details>

- **[Cross-Scale MAE]** Cross-Scale MAE: A Tale of Multi-Scale Exploitation in Remote Sensing  | **[NIPS'23]** | [`[paper]`](https://arxiv.org/abs/2401.15855) [`[code]`](https://github.com/aicip/Cross-Scale-MAE)
   <details close>
   <summary>Cross-Scale MAE Arch</summary>
   <p align="center"><img width="90%" src="https://private-user-images.githubusercontent.com/66102178/301049433-9648beeb-c371-4a42-b8ae-c49ee184f0be.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDY2Nzk3MjgsIm5iZiI6MTcwNjY3OTQyOCwicGF0aCI6Ii82NjEwMjE3OC8zMDEwNDk0MzMtOTY0OGJlZWItYzM3MS00YTQyLWI4YWUtYzQ5ZWUxODRmMGJlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAxMzElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMTMxVDA1MzcwOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJmNWE2NWM0ZTQ1NjBiMmFkYzU3MmVlZTQzMWNiOTZhZjE3ZmU0NTg4NzFmMWFiYzFkZThiMjZkMDQ4YWY3NjYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.D4H_zvkd9FJYR2AfFdN2IVcNE6Qhpel9vpOX3tqnwJw" /></p>
   </details>

- **[EVA-CLIP-18B]** EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters | **[arxiv'24]** | [`[paper]`](https://arxiv.org/abs/2402.04252) [`[code]`](https://github.com/baaivision/EVA)

- **[MIM-Refiner]** MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations | **[arxiv'24]** | [`[paper]`](https://arxiv.org/abs/2402.10093) [`[code]`](https://github.com/ml-jku/MIM-Refiner)
   <details close>
   <summary>MIM-Refiner Arch</summary>
   <p align="center"><img width="90%" src="https://private-user-images.githubusercontent.com/66102178/305323909-fa6df093-0ba7-4660-b6ed-e267bd14ec5b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDgwNjUxNTIsIm5iZiI6MTcwODA2NDg1MiwicGF0aCI6Ii82NjEwMjE3OC8zMDUzMjM5MDktZmE2ZGYwOTMtMGJhNy00NjYwLWI2ZWQtZTI2N2JkMTRlYzViLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMjE2VDA2MjczMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZjMzMxYTRkNmRiODM4Yzk5N2NkNTA0YjA1ZmM5NDY5MmYyNDlhMzhlMDljODc2YjBiMGQxZDc1YmUwOWI1MjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8TVuofJzN228rLNLxlVz8S9qbKl6gi_0qm6-L-cD0Hw" /></p>
   </details>

- **[SatMAE++]** Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery | **[CVPR'24]** | [`[paper]`](https://arxiv.org/abs/2403.05419) [`[code]`](https://github.com/techmn/satmae_pp)

- **[Augmentations vs Algorithms]** Augmentations vs Algorithms: What Works in Self-Supervised Learning | **[arxiv'24]** | [`[paper]`](https://arxiv.org/pdf/2403.05726)
  
- **[CropMAE]** Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders | **[ECCV'24]** | [`[paper]`](https://arxiv.org/abs/2403.17823) [`[code]`](https://github.com/alexandre-eymael/CropMAE)

- **[Retro]** Retro: Reusing teacher projection head for efficient embedding distillation on Lightweight Models via Self-supervised Learning | **[arxiv'24]** | [`[paper]`](https://arxiv.org/abs/2405.15311) [`[ICLR'24 under-review version]`](https://openreview.net/pdf?id=2GMTfqr7eb)

- **[ssl-data-curation]** Automatic Data Curation for Self-Supervised Learning: A Clustering-Based Approach | **[arxiv'24]** | [`[paper]`](https://arxiv.org/pdf/2405.15613) [`[code]`](https://github.com/facebookresearch/ssl-data-curation)

- **[MaSSL]** Learning from Memory: Non-Parametric Memory Augmented Self-Supervised Learning of Visual Features | **[ICML'24]** | [`[paper]`](https://arxiv.org/abs/2407.17486) [`[code]`](https://github.com/sthalles/MaSSL)

- **[SINDER]** SINDER: Repairing the Singular Defects of DINOv2 | **[ECCV'24]** | [`[paper]`](https://arxiv.org/pdf/2407.16826) [`[code]`](https://github.com/haoqiwang/sinder)

- **[MICM]** MICM: Rethinking Unsupervised Pretraining for Enhanced Few-shot Learning | **[ACM MM'24]** | [`[paper]`](https://arxiv.org/abs/2408.13385) [`[code]`](https://github.com/iCGY96/MICM)

- **[AIMv2]** Multimodal Autoregressive Pre-training of Large Vision Encoders | **[CVPR'25]** | [`[paper]`](https://arxiv.org/pdf/2411.14402) [`[code]`](https://github.com/apple/ml-aim)

- **[LIxP]** Context-Aware Multimodal Pretraining | **[CVPR'25]** | [`[paper]`](https://arxiv.org/pdf/2411.15099) 

- **[dino.txt]** DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment | **[arxiv'24]** | [`[paper]`](https://www.arxiv.org/pdf/2412.16334)

## 2025
- **[SimDINO]** Simplifying DINO via Coding Rate Regularization | **[arxiv'25]** | [`[paper]`](https://arxiv.org/abs/2502.10385) [`[code]`](https://github.com/RobinWu218/SimDINO)

- **[Web-SSL]** Scaling Language-Free Visual Representation Learning | **[arxiv'25]** | [`[paper]`](https://arxiv.org/pdf/2504.01017) [`[code]`](https://github.com/dfan/webssl)

- **[FG-CLIP]** FG-CLIP: Fine-Grained Visual and Textual Alignment | **[ICML'25]** | [`[paper]`](https://arxiv.org/abs/2505.05071) [`[code]`](https://github.com/360CVGroup/FG-CLIP)

-  **[test-time registers]** Vision Transformers Donâ€™t Need Trained Registers | **[arxiv'25]** | [`[paper]`](https://www.arxiv.org/pdf/2506.08010) [`[code]`](https://github.com/nickjiang2378/test-time-registers)

- **[AB]** Visual Pre-Training on Unlabeled Images using Reinforcement Learning | **[arxiv'25]** | [`[paper]`](https://arxiv.org/abs/2506.11967)


