# Self-Supervised Pretraining
This is a paper list of self-supervised pretrain method. All papers are listed in order of their appearance in arxiv. 

In addition, papers are also categorized according to different topics. You can click on the links below to get related papers on the topics you are interested in.


- [Self-Supervised Pretraining for Light-weight Model](Self-Supervised_Pretraining_for_Light-weight_Model.md)
- updating Â·Â·Â·


# All Papers

## 2020

- **[MoCov1]** ðŸŒŸ Momentum Contrast for Unsupervised Visual Representation Learningn | **[CVPR'20]** |[`[paper]`](https://arxiv.org/abs/1911.05722) [`[code]`](https://github.com/facebookresearch/moco) 

- **[SimCLRv1]** ðŸŒŸ A Simple Framework for Contrastive Learning of Visual Representations | **[ICML'20]** | [`[paper]`](https://arxiv.org/abs/2002.05709) [`[code]`](https://github.com/google-research/simclr)

- **[MoCov2]** Improved Baselines with Momentum Contrastive Learning | **[arxiv'20]** | [`[paper]`](https://arxiv.org/abs/2003.04297) [`[code]`](https://github.com/facebookresearch/moco)

- **[BYOL]** Bootstrap your own latent: A new approach to self-supervised Learning | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.07733) [`[code]`](https://github.com/deepmind/deepmind-research)

- **[SimCLRv2]** Big Self-Supervised Models are Strong Semi-Supervised Learners | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.10029) [`[code]`](https://github.com/google-research/simclr)

- **[SwAV]** Unsupervised Learning of Visual Features by Contrasting Cluster Assignments | **[NIPS'20]** | [`[paper]`](https://arxiv.org/abs/2006.09882) [`[code]`](https://github.com/facebookresearch/swav)

- **[RELICv1]** Representation Learning via Invariant Causal Mechanisms | **[ICLR'21]** | [`[paper]`](https://arxiv.org/abs/2010.07922)

- **[DenseCL]** Dense Contrastive Learning for Self-Supervised Visual Pre-Training | **[CVPR'21]** | [`[paper]`](https://arxiv.org/abs/2011.09157) [`[code]`](https://github.com/WXinlong/DenseCL)

- **[SimSiam]** ðŸŒŸ Exploring Simple Siamese Representation Learning | **[CVPR'21]** [`[paper]`](https://arxiv.org/abs/2011.10566) [`[code]`](https://github.com/facebookresearch/simsiam)

## 2021

- **[SEED]** SEED: Self-supervised Distillation For Visual Representation | **[ICLR'21]** | [`[paper]`](https://arxiv.org/abs/2101.04731) [`[code]`](https://github.com/zhangyifei01/SEED_ICLR21)

- **[CLIP]** ðŸŒŸ Learning Transferable Visual Models From Natural Language Supervision | **[ICML'21]** | [`[paper]`](https://arxiv.org/abs/2103.00020) [`[code]`](https://github.com/OpenAI/CLIP)

- **[Barlow Twins]** Barlow Twins: Self-Supervised Learning via Redundancy Reduction | **[ICML'21]** | [`[paper]`](https://arxiv.org/abs/2103.03230) [`[code]`](https://github.com/facebookresearch/barlowtwins)

- **[S3L]** Rethinking Self-Supervised Learning: Small is Beautiful | **[arxiv'21]** | [`[paper]`](https://arxiv.org/abs/2103.13559) [`[code]`](https://github.com/CupidJay/Scaled-down-self-supervised-learning)

- **[MoCov3]** ðŸŒŸ An Empirical Study of Training Self-Supervised Vision Transformers | **[ICCV'21]** | [`[paper]`](https://arxiv.org/abs/2104.02057) [`[code]`](https://github.com/facebookresearch/moco-v3)

- **[DisCo]** DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2104.09124) [`[code]`](https://github.com/Yuting-Gao/DisCo-pytorch)

- **[DINOv1]** ðŸŒŸ Emerging Properties in Self-Supervised Vision Transformers | **[ICCV'21]** |[`[paper]`](https://arxiv.org/abs/2104.14294) [`[code]`](https://github.com/facebookresearch/dino)

- **[VICReg]** VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2105.04906) [`[code]`](https://github.com/facebookresearch/vicreg)

- **[MST]** MST: Masked Self-Supervised Transformer for Visual Representation | **[NIPS'21]** | [`[paper]`](https://arxiv.org/abs/2106.05656) 

- **[BEiTv1]** ðŸŒŸ BEiT: BERT Pre-Training of Image Transformers | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2106.08254) [`[code]`](https://github.com/microsoft/unilm)

- **[SimDis]** Simple Distillation Baselines for Improving Small Self-supervised Models | **[ICCVW'21]** | [`[paper]`](https://arxiv.org/abs/2106.11304) [`[code]`](https://github.com/JindongGu/SimDis)

- **[OSS]** Unsupervised Representation Transfer for Small Networks: I Believe I Can Distill On-the-Fly | **[NIPS'21]** | [`[paper]`](https://proceedings.neurips.cc/paper_files/paper/2021/hash/cecd845e3577efdaaf24eea03af4c033-Abstract.html)

- **[C-BYOL/C-SimLCR]** Compressive Visual Representations | **[NIPS'21]** | [`[paper]`](https://arxiv.org/abs/2109.12909) [`[code]`](https://github.com/google-research/compressive-visual-representations)

- **[MAE]** ðŸŒŸ Masked Autoencoders Are Scalable Vision Learners | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2111.06377) [`[code]`](https://github.com/facebookresearch/mae)

- **[iBOT]** iBOT: Image BERT Pre-Training with Online Tokenizer | **[ICLR'22]** | [`[paper]`](https://arxiv.org/abs/2111.07832) [`[code]`](https://github.com/bytedance/ibot)

- **[SimMIM]** ðŸŒŸ SimMIM: A Simple Framework for Masked Image Modeling | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2111.09886) [`[code]`](https://github.com/microsoft/SimMIM)

- **[PeCo]** PeCoï¼šPerceptual Codebook for BERT Pre-training of Vision Transformers | **[AAAI'23]** | [`[paper]`](https://arxiv.org/abs/2111.12710)

- **[MaskFeat]** Masked Feature Prediction for Self-Supervised Visual Pre-Training | **[CVPR'22]** | [`[paper]`](https://arxiv.org/abs/2112.09133) [`[code]`](https://github.com/facebookresearch/SlowFast)

## 2022

- **[RELICv2]** Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet? | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2201.05119)

- **[RePre]** RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2201.06857)

- **[CAE]** Context Autoencoder for Self-Supervised Representation Learning | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2202.03026) [`[code]`](https://github.com/lxtGH/CAE)

- **[CIM]** Corrupted Image Modeling for Self-Supervised Visual Pre-Training | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2202.03382)

- **[MVP]** MVP: Multimodality-guided Visual Pre-training | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2203.05175)

- **[ConvMAE]** ConvMAE: Masked Convolution Meets Masked Autoencoders | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2205.03892) [`[code]`](https://github.com/Alpha-VL/ConvMAE)

- **[ConMIM]** Masked Image Modeling with Denoising Contrast | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2205.09616) [`[code]`](https://github.com/TencentARC/ConMIM)

- **[MixMAE]** MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers | **[CVPR'23]** |[`[paper]`](https://arxiv.org/abs/2205.13137) [`[code]`](https://github.com/Sense-X/MixMIM)

- **[A2MIM]** Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN | **[ICML'23]** | [`[paper]`](https://arxiv.org/abs/2205.13943) [`[code]`](https://github.com/Westlake-AI/A2MIM)

- **[FD]** Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14141) [`[code]`](https://github.com/SwinTransformer/Feature-Distillation)

- **[ObjMAE]** Object-wise Masked Autoencoders for Fast Pre-training | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14338)

- **[MAE-Lite]** A Closer Look at Self-Supervised Lightweight Vision Transformers | **[ICML'23]** | [`[paper]`](https://arxiv.org/abs/2205.14443) [`[code]`](https://github.com/wangsr126/mae-lite)

- **[SupMAE]** SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2205.14540) [`[code]`](https://github.com/enyac-group/supmae)

- **[HiViT]** HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2205.14949) [`[mmpretrian code]`](https://github.com/open-mmlab/mmpretrain)

- **[LoMaR]** Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2206.00790) [`[code]`](https://github.com/junchen14/LoMaR)

- **[SIM]** Siamese Image Modeling for Self-Supervised Vision Representation Learning | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2206.01204) [`[code]`](https://github.com/OpenGVLab/Siamese-Image-Modeling)

- **[MFM]** Masked Frequency Modeling for Self-Supervised Visual Pre-Training | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2206.07706) [`[code]`](https://github.com/Jiahao000/MFM)

- **[BootMAE]** Bootstrapped Masked Autoencoders for Vision BERT Pretraining | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2207.07116) [`[code]`](https://github.com/LightDXY/BootMAE)

- **[CMAE]** Contrastive Masked Autoencoders are Stronger Vision Learners | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2207.13532) [`[code]`](https://github.com/ZhichengHuang/CMAE)

- **[SMD]** Improving Self-supervised Lightweight Model Learning via Hard-aware Metric Distillation | **[ECCV'22]** | [`[paper]`](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910286.pdf) [`[code]`](https://github.com/liuhao-lh/SMD/tree/main)

- **[SdAE]** SdAE: Self-distillated Masked Autoencoder | **[ECCV'22]** | [`[paper]`](https://arxiv.org/abs/2208.00449) [`[code]`](https://github.com/AbrahamYabo/SdAE)

- **[MILAN]** MILAN: Masked Image Pretraining on Language Assisted Representation | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2208.06049) [`[code]`](https://github.com/zejiangh/MILAN)

- **[BEiTv2]** BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2208.06366) [`[code]`](https://github.com/microsoft/unilm)

- **[BEiTv3]** Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2208.10442) [`[code]`](https://github.com/microsoft/unilm)

- **[MaskCLIP]** MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2208.12262) [`[code]`](https://github.com/LightDXY/MaskCLIP)

- **[MimCo]** MimCo: Masked Image Modeling Pre-training with Contrastive Teacher | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2209.03063)

- **[VICRegL]** VICRegL: Self-Supervised Learning of Local Visual Features | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2210.01571) [`[code]`](https://github.com/facebookresearch/VICRegL)

- **[SSLight]** Effective Self-supervised Pre-training on Low-compute Networks without Distillation | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2210.02808) [`[code]`](https://github.com/saic-fi/SSLight)

- **[U-MAE]** How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders | **[NIPS'22]** | [`[paper]`](https://arxiv.org/abs/2210.08344) [`[code]`](https://github.com/zhangq327/U-MAE)

- **[i-MAE]** i-MAE: Are Latent Representations in Masked Autoencoders Linearly Separable? | **[axiv'22]** | [`[paper]`](https://arxiv.org/abs/2210.11470) [`[code]`](https://github.com/VILA-Lab/i-mae)

- **[CAN]** A simple, efficient and scalable contrastive masked autoencoder for learning visual representations | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2210.16870) [`[code]`](https://github.com/shlokk/mae-contrastive)

- **[EVA]** EVA: Exploring the Limits of Masked Visual Representation Learning at Scale | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2211.07636) [`[code]`](https://github.com/baaivision/EVA)

- **[CAEv2]** CAE v2: Context Autoencoder with CLIP Target | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2211.09799)

- **[iTPN]** Integrally Pre-Trained Transformer Pyramid Networks | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2211.12735) [`[code]`](https://github.com/sunsmarterjie/iTPN)

- **[SCFS]** Semantics-Consistent Feature Search for Self-Supervised Visual Representation Learning | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2212.06486) [`[code]`](https://github.com/skyoux/scfs)

- **[FastMIM]** FastMIM: Expediting Masked Image Modeling Pre-training for Vision | **[arxiv'22]** | [`[paper]`](https://arxiv.org/abs/2212.06593) [`[code]`](https://github.com/ggjy/FastMIM.pytorch)

- **[Scale-MAE]** Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2212.14532)

## 2023

- **[ConvNeXtv2]** ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2301.00808) [`[code]`](https://github.com/facebookresearch/ConvNeXt-V2)

- **[Spark]** Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2301.03580) [`[code]`](https://github.com/keyu-tian/SparK)

- **[I-JEPA]** Self-SupervisedÂ LearningÂ fromÂ ImagesÂ with aÂ Joint-EmbeddingÂ PredictiveÂ Architecture | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2301.08243) [`[code]`](https://github.com/facebookresearch/ijepa)

- **[RoB]** A Simple Recipe for Competitive Low-compute Self supervised Vision Models | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2301.09451)

- **[Layer Grafted]** Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations | **[ICLR'23]** | [`[paper]`](https://arxiv.org/abs/2302.14138) [`[code]`](https://github.com/VITA-Group/layerGraftedPretraining_ICLR23)

- **[PixMIM]** PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2303.02416) [`[code]`](https://github.com/open-mmlab/mmselfsup)

- **[LocalMIM]** Masked Image Modeling with Local Multi-Scale Reconstruction | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2303.05251) [`[code]`](https://github.com/huawei-noah/Efficient-Computing/tree/master/Self-supervised/LocalMIM)
 
- **[MR-MAE]** Mimic before Reconstruct: Enhancing Masked Autoencoders with Feature Mimicking | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2303.05475)

- **[MixedAE]** Mixed Autoencoder for Self-supervised Visual Representation Learning | **[CVPR'23]** | [`[paper]`](https://arxiv.org/abs/2303.17152)

- **[EMP]** EMP-SSL: Towards Self-Supervised Learning in One Training Epoch | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2304.03977) [`[code]`](https://github.com/tsb0601/EMP-SSL)

- **[DINOv2]** DINOv2ï¼šLearning Robust Visual Features without Supervision | **[arxiv'23]** | [`[paper]`](https://arxiv.org/abs/2304.07193) [`[code]`](https://github.com/facebookresearch/dinov2)

- **[CL-vs-MIM]** What Do Self-Supervised Vision Transformers Learn? | **[ICLR'23]** |[`[paper]`](https://arxiv.org/abs/2305.00729) [`[code]`](https://github.com/naver-ai/cl-vs-mim)

- **[SiamMAE]** Siamese Masked Autoencoders | **[NIPS'23]** | [`[paper]`](https://arxiv.org/abs/2305.14344)

- **[ccMIM]** Contextual Image Masking Modeling via Synergized Contrasting without View Augmentation for Faster and Better Visual Pretraining | **[ICLR'23]** | [`[paper]`](https://openreview.net/pdf?id=A3sgyt4HWp) [`[code]`](https://github.com/Sherrylone/ccMIM)

- **[DreamTeacher]** DreamTeacher: Pretraining Image Backbones with Deep Generative Models | **[ICCV'23]** |[`[paper]`](https://arxiv.org/abs/2307.07487)

- **[MFF]** Improving Pixel-based MIM by Reducing Wasted Modeling Capability | **[ICCV'23]** | [`[paper]`](https://arxiv.org/abs/2308.00261) [`[code]`](https://github.com/open-mmlab/mmpretrain)

- **[DropPos]** DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions | **[NIPS'23]** | [`[paper]`](https://arxiv.org/abs/2309.03576) [`[code]`](https://github.com/Haochen-Wang409/DropPos)
